{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Fine-tuning a pre-trained model (GoogLeNet) on iNaturalist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I will Use GoogLeNet Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:24:56.417414Z",
     "iopub.status.busy": "2025-04-17T20:24:56.416985Z",
     "iopub.status.idle": "2025-04-17T20:24:56.421195Z",
     "shell.execute_reply": "2025-04-17T20:24:56.420334Z",
     "shell.execute_reply.started": "2025-04-17T20:24:56.417386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !curl https://storage.googleapis.com/wandb_datasets/nature_12K.zip --output nature_12K.zip\n",
    "# !unzip nature_12K.zip > /dev/null 2>&1\n",
    "# !rm nature_12K.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:37:07.242283Z",
     "iopub.status.busy": "2025-04-18T11:37:07.241424Z",
     "iopub.status.idle": "2025-04-18T11:37:11.424963Z",
     "shell.execute_reply": "2025-04-18T11:37:11.424413Z",
     "shell.execute_reply.started": "2025-04-18T11:37:07.242256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = r'/kaggle/input/dl-a2-dataset/inaturalist_12K'\n",
    "split_folder = os.listdir(folder_path)\n",
    "total_data = 0\n",
    "data_per_split = []\n",
    "for folder in split_folder:\n",
    "    split_folder_path = os.path.join(folder_path, folder)\n",
    "    subtotal = 0\n",
    "    for sub_calss in os.listdir(split_folder_path):\n",
    "        files = os.listdir(os.path.join(split_folder_path, sub_calss))\n",
    "        total_data += len(files)\n",
    "        subtotal += len(files)\n",
    "    data_per_split.append([folder, subtotal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:37:14.697508Z",
     "iopub.status.busy": "2025-04-18T11:37:14.697248Z",
     "iopub.status.idle": "2025-04-18T11:37:14.701521Z",
     "shell.execute_reply": "2025-04-18T11:37:14.700987Z",
     "shell.execute_reply.started": "2025-04-18T11:37:14.697489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "[['val', 2000], ['train', 9999]]\n"
     ]
    }
   ],
   "source": [
    "print(total_data)\n",
    "print(data_per_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part B: Fine-Tunning Pretrained GoogLeNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:37:18.850169Z",
     "iopub.status.busy": "2025-04-18T11:37:18.849913Z",
     "iopub.status.idle": "2025-04-18T11:37:22.572319Z",
     "shell.execute_reply": "2025-04-18T11:37:22.571629Z",
     "shell.execute_reply.started": "2025-04-18T11:37:18.850148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:37:25.952429Z",
     "iopub.status.busy": "2025-04-18T11:37:25.952117Z",
     "iopub.status.idle": "2025-04-18T11:37:34.186743Z",
     "shell.execute_reply": "2025-04-18T11:37:34.186198Z",
     "shell.execute_reply.started": "2025-04-18T11:37:25.952406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m006\u001b[0m (\u001b[33mda24m006-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:37:34.188061Z",
     "iopub.status.busy": "2025-04-18T11:37:34.187727Z",
     "iopub.status.idle": "2025-04-18T11:37:46.480737Z",
     "shell.execute_reply": "2025-04-18T11:37:46.480176Z",
     "shell.execute_reply.started": "2025-04-18T11:37:34.188044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "load a pre-trained model and then fine-tune it using the\n",
    " naturalist data that you used in the previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T17:39:45.565997Z",
     "iopub.status.busy": "2025-04-21T17:39:45.565239Z",
     "iopub.status.idle": "2025-04-21T17:53:16.601571Z",
     "shell.execute_reply": "2025-04-21T17:53:16.600622Z",
     "shell.execute_reply.started": "2025-04-21T17:39:45.565962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m006\u001b[0m (\u001b[33mda24m006-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded with 10 classes: ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
      "100%|██████████| 49.7M/49.7M [00:00<00:00, 160MB/s] \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250421_174008-514lwbj4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/514lwbj4' target=\"_blank\">GoogLeNet-all_but_last</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/514lwbj4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/514lwbj4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5610154\n",
      "Trainable parameters: 10250 (0.18%)\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:46<00:00,  5.42it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:11<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 64.68%\n",
      "Train Loss: 1.4542, Train Acc: 55.70%, Val Loss: 1.1159, Val Acc: 64.68%\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.30it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 67.08%\n",
      "Train Loss: 1.0708, Train Acc: 65.66%, Val Loss: 1.0246, Val Acc: 67.08%\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.30it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 68.48%\n",
      "Train Loss: 1.0048, Train Acc: 67.01%, Val Loss: 0.9863, Val Acc: 68.48%\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.34it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9718, Train Acc: 68.04%, Val Loss: 0.9607, Val Acc: 68.03%\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:40<00:00,  6.23it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 68.78%\n",
      "Train Loss: 0.9360, Train Acc: 68.95%, Val Loss: 0.9465, Val Acc: 68.78%\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.36it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9144, Train Acc: 70.22%, Val Loss: 0.9584, Val Acc: 68.18%\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.29it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9096, Train Acc: 70.10%, Val Loss: 0.9687, Val Acc: 68.18%\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.36it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 69.48%\n",
      "Train Loss: 0.8984, Train Acc: 69.85%, Val Loss: 0.9472, Val Acc: 69.48%\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.26it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8470, Train Acc: 71.51%, Val Loss: 0.9398, Val Acc: 69.33%\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.28it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 70.04%\n",
      "Train Loss: 0.8557, Train Acc: 71.38%, Val Loss: 0.9355, Val Acc: 70.04%\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.33it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8577, Train Acc: 72.04%, Val Loss: 0.9361, Val Acc: 68.98%\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.32it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:10<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8438, Train Acc: 72.33%, Val Loss: 0.9304, Val Acc: 69.28%\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.38it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8520, Train Acc: 71.96%, Val Loss: 0.9390, Val Acc: 68.98%\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:40<00:00,  6.13it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:12<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8530, Train Acc: 71.31%, Val Loss: 0.9404, Val Acc: 69.13%\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [00:39<00:00,  6.39it/s]\n",
      "Validation: 100%|██████████| 63/63 [00:09<00:00,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8558, Train Acc: 71.51%, Val Loss: 0.9365, Val Acc: 69.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>final_val_acc</td><td>▁</td></tr><tr><td>learning_rate</td><td>███████▂▂▂▂▂▂▂▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>trainable_params</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▄▆▅▆▆▆▇▇█▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>70.03502</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>final_val_acc</td><td>69.43472</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>total_params</td><td>5610154</td></tr><tr><td>train_acc</td><td>71.5125</td></tr><tr><td>train_loss</td><td>0.85575</td></tr><tr><td>trainable_params</td><td>10250</td></tr><tr><td>val_acc</td><td>69.43472</td></tr><tr><td>val_loss</td><td>0.93654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GoogLeNet-all_but_last</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/514lwbj4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/514lwbj4</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250421_174008-514lwbj4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 63/63 [00:12<00:00,  5.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250421_175307-o0b5a14m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/o0b5a14m' target=\"_blank\">test_evaluation</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/o0b5a14m' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/o0b5a14m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>71.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_evaluation</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/o0b5a14m' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet/runs/o0b5a14m</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A2_PARTB_PretrainedGoogLeNet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250421_175307-o0b5a14m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy with fine-tuned model (all-but-last): 71.20%\n"
     ]
    }
   ],
   "source": [
    "# I am using GoogLeNet as Pretrained model with ImageNet weights\n",
    "# Then I am modifying the final layers for the iNaturalist dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import googlenet, GoogLeNet_Weights\n",
    "\n",
    "def setup_googlenet_model(num_classes=10):\n",
    "    \"\"\"Sets up GoogLeNet model for iNaturalist classification.\"\"\"\n",
    "    #Step 1.Load pre-trained GoogLeNet with ImageNet weights\n",
    "    # weights=GoogLeNet_Weights.IMAGENET1K_V1\n",
    "    model = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    #Step 2. GoogLeNet has 1000 output classes (for ImageNet) We need to modify it for our classes in iNaturalist\n",
    "    # Modify the final classifier layer \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    \n",
    "    #Step 3.Adapt auxiliary classifiers for training (GoogLeNet specific)\n",
    "    if hasattr(model, 'aux_logits') and model.aux_logits:\n",
    "        # Check and modify aux1 if it exists\n",
    "        if hasattr(model, 'aux1') and model.aux1 is not None:\n",
    "            if hasattr(model.aux1, 'fc'):\n",
    "                in_features = model.aux1.fc.in_features\n",
    "                model.aux1.fc = nn.Linear(in_features, num_classes)\n",
    "            elif hasattr(model.aux1, 'fc2'):\n",
    "                in_features = model.aux1.fc2.in_features\n",
    "                model.aux1.fc2 = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Check and modify aux2 if it exists\n",
    "        if hasattr(model, 'aux2') and model.aux2 is not None:\n",
    "            if hasattr(model.aux2, 'fc'):\n",
    "                in_features = model.aux2.fc.in_features\n",
    "                model.aux2.fc = nn.Linear(in_features, num_classes)\n",
    "            elif hasattr(model.aux2, 'fc2'):\n",
    "                in_features = model.aux2.fc2.in_features\n",
    "                model.aux2.fc2 = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    # Handle newer PyTorch versions where InceptionAux is used differently\n",
    "    if hasattr(model, 'AuxLogits') and model.AuxLogits is not None:\n",
    "        in_features = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fine-tuning strategies i use only last layer trainable (freze all layers except last layer) \n",
    "def freeze_layers(model, strategy='all_but_last'):\n",
    "    # First set all parameters to not require gradients (freeze all)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    if strategy == 'all_but_last':\n",
    "        # Strategy: Freeze all layers except the final classifier\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Also unfreeze auxiliary classifiers if they exist\n",
    "        if hasattr(model, 'AuxLogits') and model.AuxLogits is not None:\n",
    "            for param in model.AuxLogits.parameters():\n",
    "                param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "#PART B\n",
    "# q3_iNaturalist_dataset_preprocess.py\n",
    "# Dataset Preprocessing: \n",
    "# 1. Spliting train data in train and validation dataset\n",
    "# 2. Data augumentation on train dataset\n",
    "# 3. Creating dataloader for train and validation dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def stratified_split(dataset, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Splits dataset indices into stratified train and validation subsets.\n",
    "    :param dataset: Torch Dataset object (must have `targets` attribute).\n",
    "    :param val_split: Fraction of data to use for validation.\n",
    "    :param seed: Random seed for reproducibility.\n",
    "    :return: train_indices, val_indices\n",
    "    \"\"\"\n",
    "    targets = dataset.targets\n",
    "    label_to_indices = defaultdict(list)\n",
    "\n",
    "    # Group indices by class\n",
    "    for idx, label in enumerate(targets):\n",
    "        label_to_indices[label].append(idx)\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    for label, indices in label_to_indices.items():\n",
    "        n_total = len(indices)\n",
    "        n_val = int(n_total * val_split)\n",
    "\n",
    "        # Shuffle indices for this class\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        val_indices.extend(indices[:n_val])\n",
    "        train_indices.extend(indices[n_val:])\n",
    "\n",
    "    # Shuffle final lists (optional, especially for training)\n",
    "    random.shuffle(train_indices)\n",
    "    random.shuffle(val_indices)\n",
    "\n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "\n",
    "def prepare_inaturalist_data(data_dir, config, val_split=0.2):\n",
    "    \"\"\"\n",
    "    Prepare the iNaturalist dataset with a custom stratified split.\n",
    "    Augmentations are applied based on config.data_augmentation flag.\n",
    "    Parameters:\n",
    "    - data_dir: Path to data\n",
    "    - config: wandb.config (expects config.batch_size, config.image_size, config.data_augmentation)\n",
    "    - val_split: Fraction of training data used for validation\n",
    "    \"\"\"\n",
    "    image_size = config.image_size\n",
    "    batch_size = config.batch_size\n",
    "    \n",
    "    # Normal Transformation applied on train,valid and test all dataset\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Data Augmentation transformation  will be applied on train dataset\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load full train dataset\n",
    "    full_train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "\n",
    "    # Perform stratified split on complete train dataset to split in train and valid\n",
    "    train_indices, val_indices = stratified_split(full_train_dataset, val_split=val_split, seed=42)\n",
    "\n",
    "    # Create Subsets\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = Subset(full_train_dataset, val_indices)\n",
    "\n",
    "    # Assign  transforms conditionally\n",
    "    if config.data_augment:\n",
    "        train_dataset.dataset.transform = augment_transform\n",
    "    else:\n",
    "        train_dataset.dataset.transform = base_transform\n",
    "\n",
    "    val_dataset.dataset.transform = base_transform\n",
    "\n",
    "    \n",
    "    # Train and Validation Dataset Loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=base_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Class info\n",
    "    num_classes = len(full_train_dataset.classes)\n",
    "    class_names = full_train_dataset.classes\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_classes, class_names\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "#PART B - GoogLeNet Fine-tuning on iNaturalist dataset\n",
    "# q3_fine_tune_GoogLeNet.py\n",
    "\n",
    "#I am  fine-tuning with . freezing all layers except the last layer ()'all-but-last')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "def finetune_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"\n",
    "    Fine-tune the model using the 'all-but-last' freezing strategy.\n",
    "    Args:\n",
    "        model: The model to fine-tune\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        config: Configuration with training parameters\n",
    "        device: Device to train on (cuda/cpu)\n",
    "    Returns:\n",
    "        model: The fine-tuned model\n",
    "        history: Training and validation metrics\n",
    "    \"\"\"\n",
    "    # Initialize wandb\n",
    "    run = wandb.init(project=config.project_name, name=\"GoogLeNet-all_but_last\", config=vars(config))\n",
    "    \n",
    "    # Get parameters from config\n",
    "    num_epochs = config.num_epochs\n",
    "    learning_rate = config.learning_rate\n",
    "    weight_decay = config.weight_decay\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Log which parameters are being trained\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params} ({trainable_params/total_params:.2%})\")\n",
    "    wandb.log({\"total_params\": total_params, \"trainable_params\": trainable_params})\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Only optimize parameters that require gradients\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                          lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - handle auxiliary outputs if they exist\n",
    "            if model.training and hasattr(model, 'aux_logits') and model.aux_logits:\n",
    "                outputs, aux_outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if aux_outputs is not None:\n",
    "                    loss += 0.3 * criterion(aux_outputs, labels)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                # Handle case where model returns a tuple even when aux_logits is False\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):  # Handle auxiliary outputs\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"googlenet_all_but_last_best.pth\")\n",
    "            print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Log metrics\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        # Store in history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Log final model metrics\n",
    "    wandb.log({\n",
    "        \"final_val_acc\": val_acc,\n",
    "        \"best_val_acc\": best_val_acc\n",
    "    })\n",
    "    \n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    \n",
    "    wandb.init(project=\"DL_A2_PARTB_PretrainedGoogLeNet\", name=\"test_evaluation\")\n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    wandb.finish()\n",
    "    \n",
    "    return test_acc\n",
    "#----------------------------------\n",
    "#PART B\n",
    "#Q3 main function for fine-tuning GoogLeNet on iNaturalist dataset\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# # Import custom modules\n",
    "# from q3_iNaturalist_dataset_preprocess import  prepare_inaturalist_data\n",
    "# from q1_GoogLeNet_pretrained import setup_googlenet_model, freeze_layers\n",
    "# from q3_fine_tune_GoogLeNet import finetune_model, evaluate_model\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for model training\"\"\"\n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.batch_size = 32\n",
    "        self.image_size = 224  # GoogLeNet expects 224x224 images\n",
    "        self.data_augment = True\n",
    "        \n",
    "        # Training parameters\n",
    "        self.num_epochs = 15\n",
    "        self.learning_rate = 0.001\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        # Project name for wandb\n",
    "        self.project_name = \"DL_A2_PARTB_PretrainedGoogLeNet\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for fine-tuning GoogLeNet on iNaturalist dataset\"\"\"\n",
    "    wandb.login()\n",
    "    PROJECT_NAME = 'DL_A2_PARTB_PretrainedGoogLeNet'\n",
    "    # GPU use\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # dataset direct location i put in kaggle input directory\n",
    "    data_dir = \"/kaggle/input/dl-a2-dataset/inaturalist_12K\"\n",
    "    \n",
    "    # dataset prrprocessing and loading\n",
    "    train_loader, val_loader, test_loader, num_classes, class_names = prepare_inaturalist_data(\n",
    "        data_dir, config, val_split=0.2\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded with {num_classes} classes: {class_names}\")\n",
    "    \n",
    "    # Loading the pre-trained GoogLeNet model and  for iNaturalist dataset \n",
    "    googlenet_model = setup_googlenet_model(num_classes=10)\n",
    "    \n",
    "    # i Will use 'all-but-last' freezing strategy\n",
    "    googlenet_model = freeze_layers(googlenet_model, strategy='all_but_last')\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    fine_tuned_model, history = finetune_model(\n",
    "        googlenet_model, \n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        config,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = evaluate_model(fine_tuned_model, test_loader, device)\n",
    "    print(f\"\\nTest Accuracy with fine-tuned model (all-but-last): {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7162081,
     "sourceId": 11434609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
